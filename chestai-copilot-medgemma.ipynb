{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":118534,"databundleVersionId":14898831},{"sourceType":"modelInstanceVersion","sourceId":720206,"databundleVersionId":15326820,"modelInstanceId":547777,"modelId":480416}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import subprocess\n\nsubprocess.run(\n\n    [\"pip\", \"install\", \"-q\", \"gradio\", \"pillow\", \"accelerate\", \"transformers\"],\n\n    check=True\n\n)\n\n\n\nfrom kaggle_secrets import UserSecretsClient\n\nsecrets = UserSecretsClient()\n\nhf_token = secrets.get_secret(\"HF_TOKEN\")\n\n\n\nimport torch\n\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\n\n\nmodel_id = \"google/medgemma-4b-it\"\n\nprint(\"Loading MedGemma 1.5...\")\n\n\n\nprocessor = AutoProcessor.from_pretrained(model_id, token=hf_token)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\n    model_id,\n\n    torch_dtype=torch.bfloat16,\n\n    device_map=\"auto\",\n\n    token=hf_token\n\n)\n\nprint(\"Model loaded on:\", next(model.parameters()).device)\n\n\n\nstop_token_id = processor.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n\n\n\nclass StopOnEndOfTurn(StoppingCriteria):\n\n    def __call__(self, input_ids, scores, **kwargs):\n\n        return input_ids[0, -1].item() == stop_token_id\n\n\n\nstop_criteria = StoppingCriteriaList([StopOnEndOfTurn()])\n\n\n\n\n\ndef truncate_at_sentence(text, max_chars=600):\n\n    if len(text) <= max_chars:\n\n        garbage_phrases = [\n\n            \"example reports\", \"reference purposes\",\n\n            \"Please consult\", \"Thank You\", \"Thankyou\",\n\n            \"This response provides\", \"NOT constitute\",\n\n            \"intellectual property\", \"femtogram\",\n\n            \"nanosecond\", \"election\", \"calamit\",\n\n            \"Do Not use\", \"etcetera\", \"disclaimer\",\n\n            \"Disclaimer\", \"for your reference\"\n\n        ]\n\n        for g in garbage_phrases:\n\n            if g.lower() in text.lower():\n\n                idx = text.lower().index(g.lower())\n\n                if idx > 30:\n\n                    text = text[:idx].rstrip(\" .,;:\")\n\n                    break\n\n        return text.strip()\n\n\n\n    text = text[:max_chars]\n\n    for punct in [\". \", \".\\n\", \"! \", \"? \"]:\n\n        last = text.rfind(punct)\n\n        if last > max_chars // 2:\n\n            return text[:last + 1].strip()\n\n    return text.strip()\n\n\n\n\n\ndef generate_with_image(image_pil, prompt, max_tokens=200):\n\n    messages = [\n\n        {\n\n            \"role\": \"user\",\n\n            \"content\": [\n\n                {\"type\": \"image\", \"image\": image_pil},\n\n                {\"type\": \"text\", \"text\": prompt}\n\n            ]\n\n        }\n\n    ]\n\n    inputs = processor.apply_chat_template(\n\n        messages,\n\n        add_generation_prompt=True,\n\n        tokenize=True,\n\n        return_dict=True,\n\n        return_tensors=\"pt\"\n\n    ).to(model.device)\n\n\n\n    with torch.no_grad():\n\n        output = model.generate(\n\n            **inputs,\n\n            max_new_tokens=max_tokens,\n\n            stopping_criteria=stop_criteria,\n\n            repetition_penalty=1.2,\n\n            do_sample=False\n\n        )\n\n\n\n    new_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n\n    result = processor.tokenizer.decode(new_tokens, skip_special_tokens=True)\n\n    return truncate_at_sentence(result, max_chars=600)\n\n\n\n\n\ndef generate_text_only(prompt, max_tokens=100):\n\n    input_text = \"user\\n\" + prompt + \"\\nmodel\\n\"\n\n    inputs = processor.tokenizer(\n\n        input_text, return_tensors=\"pt\"\n\n    ).to(model.device)\n\n\n\n    with torch.no_grad():\n\n        output = model.generate(\n\n            **inputs,\n\n            max_new_tokens=max_tokens,\n\n            stopping_criteria=stop_criteria,\n\n            repetition_penalty=1.2,\n\n            do_sample=False\n\n        )\n\n\n\n    new_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n\n    result = processor.tokenizer.decode(new_tokens, skip_special_tokens=True)\n\n    return truncate_at_sentence(result, max_chars=400)\n\n\n\n\n\ndef rule_based_triage(findings, context):\n\n    \"\"\"\n\n    Rule-based triage using keyword matching on findings.\n\n    This is reliable and deterministic â€” no model hallucination.\n\n    \"\"\"\n\n    text = (findings + \" \" + context).lower()\n\n\n\n    critical_keywords = [\n\n        \"tension pneumothorax\", \"massive effusion\",\n\n        \"severe pulmonary edema\", \"aortic dissection\",\n\n        \"large pericardial effusion\", \"cardiac tamponade\",\n\n        \"complete whiteout\", \"bilateral effusion\"\n\n    ]\n\n\n\n    urgent_keywords = [\n\n        \"pneumonia\", \"consolidation\", \"opacity\", \"opacities\",\n\n        \"effusion\", \"cardiomegaly\", \"enlarged heart\",\n\n        \"cardiac enlargement\", \"pacemaker\", \"enlarged cardiac\",\n\n        \"atelectasis\", \"mass\", \"nodule\", \"infiltrate\",\n\n        \"pleural fluid\", \"heart failure\", \"edema\",\n\n        \"borderline enlarged\", \"upper limits\", \"prominent\",\n\n        \"increased density\", \"haziness\", \"airspace\"\n\n    ]\n\n\n\n    for kw in critical_keywords:\n\n        if kw in text:\n\n            return \"CRITICAL\"\n\n\n\n    for kw in urgent_keywords:\n\n        if kw in text:\n\n            return \"URGENT\"\n\n\n\n    return \"ROUTINE\"\n\n\n\n\n\nimport gradio as gr\n\nfrom PIL import Image\n\nimport time\n\n\n\n\n\ndef run_pipeline(image, patient_context):\n\n    if image is None:\n\n        return (\"Please upload an image\", \"\", \"\", \"\", \"\", \"\")\n\n\n\n    if not patient_context:\n\n        patient_context = \"No clinical context provided\"\n\n\n\n    log = []\n\n    start_total = time.time()\n\n    img_pil = Image.fromarray(image).convert(\"RGB\").resize((512, 512))\n\n\n\n    # STEP 1: Visual Findings â€” MedGemma does the real medical work here\n\n    t = time.time()\n\n    log.append(\"Step 1: Extracting visual findings with MedGemma...\")\n\n\n\n    findings = generate_with_image(\n\n        img_pil,\n\n        \"You are an expert radiologist. Carefully analyze this chest X-ray.\\n\"\n\n        \"Patient context: \" + patient_context + \"\\n\\n\"\n\n        \"Describe findings for each region below. \"\n\n        \"Be specific about any abnormalities you see. \"\n\n        \"Write exactly 5 lines and stop:\\n\"\n\n        \"Lungs: [describe findings or Normal]\\n\"\n\n        \"Heart: [describe size and any findings or Normal]\\n\"\n\n        \"Mediastinum: [describe or Normal]\\n\"\n\n        \"Pleura: [describe or Normal]\\n\"\n\n        \"Bones: [describe or Normal]\",\n\n        250\n\n    )\n\n\n\n    log.append(\"Step 1 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 2: Triage â€” rule-based on findings keywords (reliable)\n\n    t = time.time()\n\n    log.append(\"Step 2: Classifying triage level...\")\n\n\n\n    triage_level = rule_based_triage(findings, patient_context)\n\n\n\n    triage_descriptions = {\n\n        \"CRITICAL\": \"Critical findings identified requiring immediate radiologist attention.\",\n\n        \"URGENT\": \"Significant findings identified requiring prompt radiologist review.\",\n\n        \"ROUTINE\": \"No acute findings identified. Standard reporting timeline applies.\"\n\n    }\n\n    triage_reason = triage_descriptions[triage_level]\n\n\n\n    log.append(\"Step 2 complete: \" + triage_level + \" (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 3: Report â€” MedGemma generates the findings paragraph\n\n    t = time.time()\n\n    log.append(\"Step 3: Generating radiology report with MedGemma...\")\n\n\n\n    findings_paragraph = generate_text_only(\n\n        \"You are a radiologist. Write 2 professional sentences \"\n\n        \"summarizing these chest X-ray findings. \"\n\n        \"Be concise and use medical terminology. \"\n\n        \"Stop after exactly 2 sentences.\\n\\n\"\n\n        \"Findings: \" + findings[:350],\n\n        90\n\n    )\n\n\n\n    impression_map = {\n\n        \"CRITICAL\": (\n\n            \"1. Critical cardiopulmonary finding identified.\\n\"\n\n            \"2. Immediate radiologist review and clinical intervention required.\"\n\n        ),\n\n        \"URGENT\": (\n\n            \"1. Significant cardiopulmonary finding identified.\\n\"\n\n            \"2. Prompt radiologist review and clinical correlation recommended.\"\n\n        ),\n\n        \"ROUTINE\": (\n\n            \"1. No acute cardiopulmonary abnormality identified.\\n\"\n\n            \"2. Findings within normal limits for patient age and clinical context.\"\n\n        )\n\n    }\n\n\n\n    recommendation_map = {\n\n        \"CRITICAL\": \"Immediate radiologist review and clinical intervention required.\",\n\n        \"URGENT\": \"Radiologist review recommended within 1 hour. Clinical correlation advised.\",\n\n        \"ROUTINE\": \"No immediate follow-up required. Routine clinical management.\"\n\n    }\n\n\n\n    report = (\n\n        \"**EXAMINATION:** PA chest radiograph\\n\\n\"\n\n        \"**INDICATION:** \" + patient_context + \"\\n\\n\"\n\n        \"**COMPARISON:** None available\\n\\n\"\n\n        \"**FINDINGS:** \" + findings_paragraph + \"\\n\\n\"\n\n        \"**IMPRESSION:**\\n\" + impression_map[triage_level] + \"\\n\\n\"\n\n        \"**RECOMMENDATION:** \" + recommendation_map[triage_level]\n\n    )\n\n\n\n    log.append(\"Step 3 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 4: Patient Summary â€” templated for reliability\n\n    t = time.time()\n\n    log.append(\"Step 4: Creating patient summary...\")\n\n\n\n    patient_summaries = {\n\n        \"CRITICAL\": (\n\n            \"Your chest X-ray has shown some findings that need urgent attention \"\n\n            \"from your doctor right away. \"\n\n            \"Please do not leave the hospital â€” your care team has been alerted \"\n\n            \"and will see you very soon. \"\n\n            \"This does not mean the worst, but it is important we act quickly \"\n\n            \"to keep you safe.\"\n\n        ),\n\n        \"URGENT\": (\n\n            \"Your chest X-ray has shown some findings that your doctor needs \"\n\n            \"to review soon. \"\n\n            \"This is not an emergency, but your care team will follow up with \"\n\n            \"you shortly to discuss next steps. \"\n\n            \"Please let a nurse know if your symptoms get worse while you wait.\"\n\n        ),\n\n        \"ROUTINE\": (\n\n            \"Your chest X-ray looks generally normal and no urgent problems \"\n\n            \"were found. \"\n\n            \"Your doctor will review the full results with you at your appointment. \"\n\n            \"If your symptoms change or worsen before then, please contact \"\n\n            \"your healthcare provider.\"\n\n        )\n\n    }\n\n    patient_summary = patient_summaries[triage_level]\n\n\n\n    log.append(\"Step 4 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 5: Safety flags\n\n    t = time.time()\n\n    log.append(\"Step 5: Running safety validation...\")\n\n\n\n    safety_flags = []\n\n    if triage_level == \"CRITICAL\":\n\n        safety_flags.append(\"ðŸ”´ CRITICAL - Immediate radiologist review required\")\n\n    elif triage_level == \"URGENT\":\n\n        safety_flags.append(\"ðŸŸ¡ URGENT - Radiologist review within 1 hour\")\n\n    else:\n\n        safety_flags.append(\"ðŸŸ¢ ROUTINE - Standard reporting timeline\")\n\n    safety_flags.append(\"âš ï¸ AI-generated draft - Must be verified by a qualified radiologist\")\n\n    safety_flags.append(\"âš ï¸ For demonstration purposes only - Not a medical device\")\n\n\n\n    log.append(\"Step 5 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n    log.append(\"Total pipeline time: \" + str(round(time.time()-start_total, 1)) + \"s\")\n\n\n\n    emoji_map = {\"CRITICAL\": \"ðŸ”´\", \"URGENT\": \"ðŸŸ¡\", \"ROUTINE\": \"ðŸŸ¢\"}\n\n    emoji = emoji_map.get(triage_level, \"âšª\")\n\n\n\n    return (\n\n        \"## \" + emoji + \" \" + triage_level + \"\\n\\n\" + triage_reason,\n\n        \"## Detailed Findings\\n\\n\" + findings,\n\n        \"## Radiology Report\\n\\n\" + report,\n\n        \"## Patient Summary\\n\\n\" + patient_summary,\n\n        \"\\n\".join(safety_flags),\n\n        \"\\n\".join(log)\n\n    )\n\n\n\n\n\nwith gr.Blocks(title=\"ChestAI Copilot\") as demo:\n\n\n\n    gr.Markdown(\n\n        \"# ðŸ« ChestAI Copilot\\n\"\n\n        \"### Agentic Chest X-Ray Triage and Reporting Assistant\\n\"\n\n        \"**Powered by Google MedGemma 1.5 (HAI-DEF)** | **MedGemma Impact Challenge**\\n\\n\"\n\n        \"> âš ï¸ Demonstration only. Not a medical device. \"\n\n        \"All outputs must be verified by a qualified radiologist.\\n\\n\"\n\n        \"---\\n\\n\"\n\n        \"**How it works:** Upload a chest X-ray and the 5-step \"\n\n        \"AI agent pipeline produces Triage + Report + Patient Summary\"\n\n    )\n\n\n\n    with gr.Row():\n\n        with gr.Column(scale=1):\n\n            img_input = gr.Image(\n\n                label=\"ðŸ“¤ Upload Chest X-Ray\",\n\n                type=\"numpy\",\n\n                height=350\n\n            )\n\n            context_input = gr.Textbox(\n\n                label=\"ðŸ“‹ Clinical Context\",\n\n                placeholder=\"e.g., 65yo male, cough and fever x3 days, history of CHF\",\n\n                lines=3\n\n            )\n\n            gr.Markdown(\n\n                \"**Test contexts:**\\n\"\n\n                \"- 72yo female, shortness of breath, history of heart failure\\n\"\n\n                \"- 55yo male, fever, cough, low oxygen saturation x3 days\\n\"\n\n                \"- 28yo female, routine pre-employment screening\"\n\n            )\n\n            submit_btn = gr.Button(\n\n                \"ðŸ” Analyze Chest X-Ray\",\n\n                variant=\"primary\",\n\n                size=\"lg\"\n\n            )\n\n\n\n        with gr.Column(scale=2):\n\n            with gr.Row():\n\n                triage_output = gr.Markdown(label=\"Triage Level\")\n\n                safety_output = gr.Textbox(label=\"Safety Flags\", lines=4)\n\n            with gr.Tabs():\n\n                with gr.TabItem(\"ðŸ“‹ Radiology Report\"):\n\n                    report_output = gr.Markdown()\n\n                with gr.TabItem(\"ðŸ” Detailed Findings\"):\n\n                    findings_output = gr.Markdown()\n\n                with gr.TabItem(\"ðŸ’¬ Patient Summary\"):\n\n                    patient_output = gr.Markdown()\n\n                with gr.TabItem(\"ðŸ“ Pipeline Log\"):\n\n                    log_output = gr.Textbox(lines=10, label=\"Agent Execution Log\")\n\n\n\n    gr.Markdown(\n\n        \"---\\n\"\n\n        \"### ðŸ—ï¸ Architecture\\n\"\n\n        \"```\\n\"\n\n        \"Upload CXR -> [Agent 1: Visual Analysis (MedGemma)] \"\n\n        \"-> [Agent 2: Triage Classification] \"\n\n        \"-> [Agent 3: Report Generation (MedGemma)] \"\n\n        \"-> [Agent 4: Patient Summary] \"\n\n        \"-> [Agent 5: Safety Validation] -> Dashboard\\n\"\n\n        \"```\\n\\n\"\n\n        \"**Model:** MedGemma 1.5 4B-IT (multimodal) from Google HAI-DEF\\n\\n\"\n\n        \"*Built for the MedGemma Impact Challenge on Kaggle*\"\n\n    )\n\n\n\n    submit_btn.click(\n\n        fn=run_pipeline,\n\n        inputs=[img_input, context_input],\n\n        outputs=[\n\n            triage_output,\n\n            findings_output,\n\n            report_output,\n\n            patient_output,\n\n            safety_output,\n\n            log_output\n\n        ]\n\n    )\n\n\n\ndemo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T15:59:53.258992Z","iopub.execute_input":"2026-02-24T15:59:53.259481Z"}},"outputs":[{"name":"stdout","text":"Loading MedGemma 1.5...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbd38288e8694417850fd1831efd44b8"}},"metadata":{}},{"name":"stdout","text":"Model loaded on: cuda:0\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://0db1945916862a8676.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://0db1945916862a8676.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stderr","text":"ERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 416, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1139, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 107, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n    raise exc\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n    await self.app(scope, receive, _send)\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n    await route.handle(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 119, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 105, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 385, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 284, in run_endpoint_function\n    return await dependant.call(**values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n    await asyncio.wait_for(\n  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n    return await fut\n           ^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n    return await self._signals[upload_id].wait()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n    fut = self._get_loop().create_future()\n          ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n    raise RuntimeError(f'{self!r} is bound to a different event loop')\nRuntimeError: <asyncio.locks.Event object at 0x7896a0bbd880 [unset]> is bound to a different event loop\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 416, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1139, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 107, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n    raise exc\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n    await self.app(scope, receive, _send)\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n    await route.handle(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 119, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 105, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 385, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 284, in run_endpoint_function\n    return await dependant.call(**values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n    await asyncio.wait_for(\n  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n    return await fut\n           ^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n    return await self._signals[upload_id].wait()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n    fut = self._get_loop().create_future()\n          ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n    raise RuntimeError(f'{self!r} is bound to a different event loop')\nRuntimeError: <asyncio.locks.Event object at 0x7896a0bbd880 [unset]> is bound to a different event loop\n","output_type":"stream"}],"execution_count":null}]}
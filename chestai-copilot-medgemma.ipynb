{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":118534,"databundleVersionId":14898831},{"sourceType":"modelInstanceVersion","sourceId":720206,"databundleVersionId":15326820,"modelInstanceId":547777,"modelId":480416}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import subprocess\n\n\n\n# Uninstall old gradio first, then install latest\n\nsubprocess.run([\"pip\", \"uninstall\", \"-y\", \"gradio\", \"gradio-client\"], check=False)\n\nsubprocess.run([\"pip\", \"install\", \"-q\", \"gradio\", \"pillow\", \"accelerate\", \"transformers\"], check=True)\n\n\n\nfrom kaggle_secrets import UserSecretsClient\n\nsecrets = UserSecretsClient()\n\nhf_token = secrets.get_secret(\"HF_TOKEN\")\n\n\n\nimport torch\n\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\nimport numpy as np\n\nfrom PIL import Image\n\nimport time\n\nimport gradio as gr\n\nimport re\n\n\n\nprint(\"Gradio version:\", gr.__version__)\n\n\n\nmodel_id = \"google/medgemma-4b-it\"\n\nprint(\"Loading MedGemma 1.5...\")\n\n\n\nprocessor = AutoProcessor.from_pretrained(model_id, token=hf_token)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\n    model_id,\n\n    torch_dtype=torch.bfloat16,\n\n    device_map=\"auto\",\n\n    token=hf_token\n\n)\n\nprint(\"Model loaded on:\", next(model.parameters()).device)\n\n\n\nstop_token_id = processor.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n\n\n\nclass StopOnEndOfTurn(StoppingCriteria):\n\n    def __call__(self, input_ids, scores, **kwargs):\n\n        return input_ids[0, -1].item() == stop_token_id\n\n\n\nstop_criteria = StoppingCriteriaList([StopOnEndOfTurn()])\n\n\n\n\n\ndef truncate_at_sentence(text, max_chars=600):\n\n    # Clean up MedGemma QA artifacts (e.g. \\boxed{})\n\n    text = re.sub(r'(?i)final answer:.*', '', text)\n\n    text = re.sub(r'(?i)the final answer is.*', '', text)\n\n    text = text.replace('\\\\boxed{', '').replace('}', '')\n\n    text = text.strip()\n\n\n\n    if len(text) <= max_chars:\n\n        garbage_phrases = [\n\n            \"example reports\", \"reference purposes\",\n\n            \"Please consult\", \"Thank You\", \"Thankyou\",\n\n            \"This response provides\", \"NOT constitute\",\n\n            \"intellectual property\", \"femtogram\",\n\n            \"nanosecond\", \"election\", \"calamit\",\n\n            \"Do Not use\", \"etcetera\", \"disclaimer\",\n\n            \"Disclaimer\", \"for your reference\"\n\n        ]\n\n        for g in garbage_phrases:\n\n            if g.lower() in text.lower():\n\n                idx = text.lower().index(g.lower())\n\n                if idx > 30:\n\n                    text = text[:idx].rstrip(\" .,;:\")\n\n                    break\n\n        return text.strip()\n\n\n\n    text = text[:max_chars]\n\n    for punct in [\". \", \".\\n\", \"! \", \"? \"]:\n\n        last = text.rfind(punct)\n\n        if last > max_chars // 2:\n\n            return text[:last + 1].strip()\n\n    return text.strip()\n\n\n\n\n\ndef generate_with_image(image_pil, prompt, max_tokens=200):\n\n    messages = [\n\n        {\n\n            \"role\": \"user\",\n\n            \"content\": [\n\n                {\"type\": \"image\", \"image\": image_pil},\n\n                {\"type\": \"text\", \"text\": prompt}\n\n            ]\n\n        }\n\n    ]\n\n    inputs = processor.apply_chat_template(\n\n        messages,\n\n        add_generation_prompt=True,\n\n        tokenize=True,\n\n        return_dict=True,\n\n        return_tensors=\"pt\"\n\n    ).to(model.device)\n\n\n\n    with torch.no_grad():\n\n        output = model.generate(\n\n            **inputs,\n\n            max_new_tokens=max_tokens,\n\n            stopping_criteria=stop_criteria,\n\n            repetition_penalty=1.2,\n\n            do_sample=False\n\n        )\n\n\n\n    new_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n\n    result = processor.tokenizer.decode(new_tokens, skip_special_tokens=True)\n\n    return truncate_at_sentence(result, max_chars=600)\n\n\n\n\n\ndef generate_text_only(prompt, max_tokens=100):\n\n    input_text = \"user\\n\" + prompt + \"\\nmodel\\n\"\n\n    inputs = processor.tokenizer(\n\n        input_text, return_tensors=\"pt\"\n\n    ).to(model.device)\n\n\n\n    with torch.no_grad():\n\n        output = model.generate(\n\n            **inputs,\n\n            max_new_tokens=max_tokens,\n\n            stopping_criteria=stop_criteria,\n\n            repetition_penalty=1.2,\n\n            do_sample=False\n\n        )\n\n\n\n    new_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n\n    result = processor.tokenizer.decode(new_tokens, skip_special_tokens=True)\n\n    return truncate_at_sentence(result, max_chars=400)\n\n\n\n\n\ndef rule_based_triage(findings, context):\n\n    findings_text = findings.lower()\n\n    context_text = context.lower()\n\n    combined = findings_text + \" \" + context_text\n\n\n\n    # Smart regex-based negation checker\n\n    def has_positive_mention(keywords, text):\n\n        for kw in keywords:\n\n            # Find all matches of the keyword in the text\n\n            for match in re.finditer(re.escape(kw), text):\n\n                # Look at the 60 characters preceding the keyword\n\n                preceding = text[max(0, match.start() - 60):match.start()]\n\n                # Split by punctuation to only look at the current clause/sentence\n\n                preceding_clause = re.split(r'[.;\\n]', preceding)[-1]\n\n                \n\n                # If any of these negation words appear in the same clause before the keyword, ignore it\n\n                negations = [\"no \", \"not \", \"without \", \"negative \", \"absence \", \"unremarkable \", \"clear \", \"free of \", \"zero \"]\n\n                is_negated = any(neg in preceding_clause for neg in negations)\n\n                \n\n                # If we found the keyword and it wasn't negated, we have a positive finding!\n\n                if not is_negated:\n\n                    return True\n\n        return False\n\n\n\n    # CRITICAL â€” always overrides\n\n    critical_keywords = [\n\n        \"tension pneumothorax\", \"massive pleural effusion\", \"severe pulmonary edema\",\n\n        \"aortic dissection\", \"cardiac tamponade\", \"complete whiteout\", \"bilateral pleural effusion\"\n\n    ]\n\n    if has_positive_mention(critical_keywords, combined):\n\n        return \"CRITICAL\"\n\n\n\n    # High risk clinical context keywords\n\n    high_risk_context = [\n\n        \"low oxygen\", \"oxygen saturation\", \"hypoxia\", \"shortness of breath\",\n\n        \"difficulty breathing\", \"chest pain\", \"heart failure\", \"chf\",\n\n        \"trauma\", \"accident\", \"fever\", \"cough\"\n\n    ]\n\n\n\n    # Normal indicators in FINDINGS only\n\n    normal_indicators = [\n\n        \"no acute\", \"within normal\", \"no evidence of\", \"unremarkable\",\n\n        \"clear bilaterally\", \"no abnormality\", \"normal limits\",\n\n        \"no significant\", \"appears normal\", \"no consolidation\",\n\n        \"no effusion\", \"no pneumothorax\", \"lungs are clear\",\n\n        \"lungs appear clear\", \"heart size is normal\", \"normal heart size\",\n\n        \"no cardiomegaly\", \"no masses\", \"no nodules\",\n\n        \"normal vascular markings\", \"no acute findings\",\n\n        \"grossly normal\", \"no pleural\"\n\n    ]\n\n    normal_count = sum(1 for phrase in normal_indicators if phrase in findings_text)\n\n\n\n    # URGENT keywords in findings\n\n    urgent_keywords = [\n\n        \"consolidation\", \"pneumonia\", \"pleural effusion\", \"cardiomegaly\",\n\n        \"enlarged heart\", \"cardiac enlargement\", \"enlarged cardiac silhouette\",\n\n        \"pacemaker\", \"pulmonary edema\", \"airspace opacity\", \"lobar opacity\",\n\n        \"infiltrate\", \"lobar atelectasis\", \"pleural fluid\", \"ground glass opacity\",\n\n        \"mass identified\", \"nodule identified\", \"borderline enlarged\",\n\n        \"upper limits of normal\", \"increased opacity\", \"haziness\", \"opacification\"\n\n    ]\n\n\n\n    urgent_in_findings = has_positive_mention(urgent_keywords, findings_text)\n\n    high_risk_in_context = any(kw in context_text for kw in high_risk_context)\n\n\n\n    # URGENT Logic\n\n    if urgent_in_findings:\n\n        return \"URGENT\"\n\n\n\n    if high_risk_in_context and normal_count < 3:\n\n        return \"URGENT\"\n\n\n\n    return \"ROUTINE\"\n\n\n\n\n\ndef run_pipeline(image, patient_context):\n\n    if image is None:\n\n        return (\"Please upload an image\", \"\", \"\", \"\", \"\", \"\")\n\n\n\n    if not patient_context or patient_context.strip() == \"\":\n\n        patient_context = \"No clinical context provided\"\n\n\n\n    log = []\n\n    start_total = time.time()\n\n\n\n    try:\n\n        if isinstance(image, np.ndarray):\n\n            img_pil = Image.fromarray(image.astype(\"uint8\")).convert(\"RGB\")\n\n        else:\n\n            img_pil = image.convert(\"RGB\")\n\n        img_pil = img_pil.resize((512, 512))\n\n    except Exception as e:\n\n        return (\"Error loading image: \" + str(e), \"\", \"\", \"\", \"\", \"\")\n\n\n\n    # STEP 1\n\n    t = time.time()\n\n    log.append(\"Step 1: Extracting visual findings with MedGemma...\")\n\n    try:\n\n        findings = generate_with_image(\n\n            img_pil,\n\n            \"You are an expert radiologist. Carefully analyze this chest X-ray.\\n\"\n\n            \"Patient context: \" + patient_context + \"\\n\\n\"\n\n            \"Write exactly 5 lines, one per region, then stop:\\n\"\n\n            \"Lungs: [describe findings or write Normal]\\n\"\n\n            \"Heart: [describe size and findings or write Normal]\\n\"\n\n            \"Mediastinum: [describe or write Normal]\\n\"\n\n            \"Pleura: [describe or write Normal]\\n\"\n\n            \"Bones: [describe or write Normal]\\n\\n\"\n\n            \"Be specific. If you see consolidation, effusion, \"\n\n            \"cardiomegaly, or pacemaker, name it explicitly.\",\n\n            250\n\n        )\n\n    except Exception as e:\n\n        findings = \"Error in visual analysis: \" + str(e)\n\n    log.append(\"Step 1 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 2\n\n    t = time.time()\n\n    log.append(\"Step 2: Classifying triage level...\")\n\n    triage_level = rule_based_triage(findings, patient_context)\n\n    triage_descriptions = {\n\n        \"CRITICAL\": \"Critical findings identified requiring immediate radiologist attention.\",\n\n        \"URGENT\": \"Significant findings identified requiring prompt radiologist review.\",\n\n        \"ROUTINE\": \"No acute findings identified. Standard reporting timeline applies.\"\n\n    }\n\n    triage_reason = triage_descriptions[triage_level]\n\n    log.append(\"Step 2 complete: \" + triage_level + \" (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 3\n\n    t = time.time()\n\n    log.append(\"Step 3: Generating radiology report with MedGemma...\")\n\n    try:\n\n        findings_paragraph = generate_text_only(\n\n            \"You are a radiologist. Write exactly 2 professional sentences \"\n\n            \"summarizing these chest X-ray findings. \"\n\n            \"Stop after 2 sentences. No extra commentary.\\n\\n\"\n\n            \"Findings: \" + findings[:350],\n\n            90\n\n        )\n\n    except Exception as e:\n\n        findings_paragraph = findings[:200]\n\n\n\n    impression_map = {\n\n        \"CRITICAL\": (\n\n            \"1. Critical cardiopulmonary finding identified.\\n\"\n\n            \"2. Immediate radiologist review and clinical intervention required.\"\n\n        ),\n\n        \"URGENT\": (\n\n            \"1. Significant cardiopulmonary finding identified.\\n\"\n\n            \"2. Prompt radiologist review and clinical correlation recommended.\"\n\n        ),\n\n        \"ROUTINE\": (\n\n            \"1. No acute cardiopulmonary abnormality identified.\\n\"\n\n            \"2. Findings within normal limits for patient age and clinical context.\"\n\n        )\n\n    }\n\n    recommendation_map = {\n\n        \"CRITICAL\": \"Immediate radiologist review and clinical intervention required.\",\n\n        \"URGENT\": \"Radiologist review recommended within 1 hour. Clinical correlation advised.\",\n\n        \"ROUTINE\": \"No immediate follow-up required. Routine clinical management.\"\n\n    }\n\n    report = (\n\n        \"**EXAMINATION:** PA chest radiograph\\n\\n\"\n\n        \"**INDICATION:** \" + patient_context + \"\\n\\n\"\n\n        \"**COMPARISON:** None available\\n\\n\"\n\n        \"**FINDINGS:** \" + findings_paragraph + \"\\n\\n\"\n\n        \"**IMPRESSION:**\\n\" + impression_map[triage_level] + \"\\n\\n\"\n\n        \"**RECOMMENDATION:** \" + recommendation_map[triage_level]\n\n    )\n\n    log.append(\"Step 3 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 4\n\n    t = time.time()\n\n    log.append(\"Step 4: Creating patient summary...\")\n\n    patient_summaries = {\n\n        \"CRITICAL\": (\n\n            \"Your chest X-ray has shown some findings that need urgent attention \"\n\n            \"from your doctor right away. \"\n\n            \"Please do not leave the hospital â€” your care team has been alerted \"\n\n            \"and will see you very soon. \"\n\n            \"This does not mean the worst, but it is important we act quickly \"\n\n            \"to keep you safe.\"\n\n        ),\n\n        \"URGENT\": (\n\n            \"Your chest X-ray has shown some findings that your doctor needs \"\n\n            \"to review soon. \"\n\n            \"This is not an emergency, but your care team will follow up with \"\n\n            \"you shortly to discuss next steps. \"\n\n            \"Please let a nurse know if your symptoms get worse while you wait.\"\n\n        ),\n\n        \"ROUTINE\": (\n\n            \"Your chest X-ray looks generally normal and no urgent problems \"\n\n            \"were found. \"\n\n            \"Your doctor will review the full results with you at your appointment. \"\n\n            \"If your symptoms change or worsen before then, please contact \"\n\n            \"your healthcare provider.\"\n\n        )\n\n    }\n\n    patient_summary = patient_summaries[triage_level]\n\n    log.append(\"Step 4 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 5\n\n    t = time.time()\n\n    log.append(\"Step 5: Running safety validation...\")\n\n    safety_flags = []\n\n    if triage_level == \"CRITICAL\":\n\n        safety_flags.append(\"CRITICAL - Immediate radiologist review required\")\n\n    elif triage_level == \"URGENT\":\n\n        safety_flags.append(\"URGENT - Radiologist review within 1 hour\")\n\n    else:\n\n        safety_flags.append(\"ROUTINE - Standard reporting timeline\")\n\n    safety_flags.append(\"AI-generated draft - Must be verified by a qualified radiologist\")\n\n    safety_flags.append(\"For demonstration purposes only - Not a medical device\")\n\n    log.append(\"Step 5 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n    log.append(\"Total pipeline time: \" + str(round(time.time()-start_total, 1)) + \"s\")\n\n\n\n    emoji_map = {\"CRITICAL\": \"ðŸ”´\", \"URGENT\": \"ðŸŸ¡\", \"ROUTINE\": \"ðŸŸ¢\"}\n\n    emoji = emoji_map.get(triage_level, \"âšª\")\n\n\n\n    return (\n\n        \"## \" + emoji + \" \" + triage_level + \"\\n\\n\" + triage_reason,\n\n        \"## Detailed Findings\\n\\n\" + findings,\n\n        \"## Radiology Report\\n\\n\" + report,\n\n        \"## Patient Summary\\n\\n\" + patient_summary,\n\n        \"\\n\".join(safety_flags),\n\n        \"\\n\".join(log)\n\n    )\n\n\n\n\n\nwith gr.Blocks(title=\"ChestAI Copilot\") as demo:\n\n    gr.Markdown(\n\n        \"# ChestAI Copilot\\n\"\n\n        \"### Agentic Chest X-Ray Triage and Reporting Assistant\\n\"\n\n        \"**Powered by Google MedGemma 1.5 (HAI-DEF)** | **MedGemma Impact Challenge**\\n\\n\"\n\n        \"> Demonstration only. Not a medical device. \"\n\n        \"All outputs must be verified by a qualified radiologist.\\n\\n\"\n\n        \"---\\n\\n\"\n\n        \"**How it works:** Upload a chest X-ray and the 5-step \"\n\n        \"AI agent pipeline produces Triage + Report + Patient Summary\"\n\n    )\n\n\n\n    with gr.Row():\n\n        with gr.Column(scale=1):\n\n            img_input = gr.Image(\n\n                label=\"Upload Chest X-Ray\",\n\n                type=\"numpy\",\n\n                height=300\n\n            )\n\n            context_input = gr.Textbox(\n\n                label=\"Clinical Context\",\n\n                placeholder=\"e.g., 65yo male, cough and fever x3 days, history of CHF\",\n\n                lines=3\n\n            )\n\n            gr.Markdown(\n\n                \"**Test contexts:**\\n\"\n\n                \"- 72yo female, shortness of breath, history of heart failure\\n\"\n\n                \"- 55yo male, fever, cough, low oxygen saturation x3 days\\n\"\n\n                \"- 28yo female, routine pre-employment screening\"\n\n            )\n\n            submit_btn = gr.Button(\"Analyze Chest X-Ray\", variant=\"primary\")\n\n\n\n        with gr.Column(scale=2):\n\n            triage_output = gr.Markdown(label=\"Triage Level\")\n\n            safety_output = gr.Textbox(label=\"Safety Flags\", lines=3)\n\n            with gr.Tabs():\n\n                with gr.TabItem(\"Radiology Report\"):\n\n                    report_output = gr.Markdown()\n\n                with gr.TabItem(\"Detailed Findings\"):\n\n                    findings_output = gr.Markdown()\n\n                with gr.TabItem(\"Patient Summary\"):\n\n                    patient_output = gr.Markdown()\n\n                with gr.TabItem(\"Pipeline Log\"):\n\n                    log_output = gr.Textbox(lines=10, label=\"Agent Execution Log\")\n\n\n\n    gr.Markdown(\n\n        \"---\\n\"\n\n        \"### Architecture\\n\"\n\n        \"```\\n\"\n\n        \"Upload CXR -> [Agent 1: Visual Analysis (MedGemma)] \"\n\n        \"-> [Agent 2: Triage Classification] \"\n\n        \"-> [Agent 3: Report Generation (MedGemma)] \"\n\n        \"-> [Agent 4: Patient Summary] \"\n\n        \"-> [Agent 5: Safety Validation] -> Dashboard\\n\"\n\n        \"```\\n\\n\"\n\n        \"**Model:** MedGemma 1.5 4B-IT (multimodal) from Google HAI-DEF\\n\\n\"\n\n        \"*Built for the MedGemma Impact Challenge on Kaggle*\"\n\n    )\n\n\n\n    submit_btn.click(\n\n        fn=run_pipeline,\n\n        inputs=[img_input, context_input],\n\n        outputs=[\n\n            triage_output,\n\n            findings_output,\n\n            report_output,\n\n            patient_output,\n\n            safety_output,\n\n            log_output\n\n        ]\n\n    )\n\n\n\ndemo.queue().launch(share=True, debug=True, show_error=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:49:55.706328Z","iopub.execute_input":"2026-02-24T17:49:55.706630Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: gradio 6.6.0\nUninstalling gradio-6.6.0:\n  Successfully uninstalled gradio-6.6.0\nFound existing installation: gradio_client 2.1.0\nUninstalling gradio_client-2.1.0:\n  Successfully uninstalled gradio_client-2.1.0\nGradio version: 6.6.0\nLoading MedGemma 1.5...\n","output_type":"stream"},{"name":"stderr","text":"The image processor of type `Gemma3ImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f28a40cf514950b6ac90521212dac1"}},"metadata":{}},{"name":"stdout","text":"Model loaded on: cuda:0\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://a5b1a146af366f948c.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://a5b1a146af366f948c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":null}]}
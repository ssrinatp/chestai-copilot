{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":118534,"databundleVersionId":14898831},{"sourceType":"modelInstanceVersion","sourceId":720206,"databundleVersionId":15326820,"modelInstanceId":547777,"modelId":480416}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import subprocess\n\n\n\n# Uninstall old gradio first, then install latest\n\nsubprocess.run([\"pip\", \"uninstall\", \"-y\", \"gradio\", \"gradio-client\"], check=False)\n\nsubprocess.run([\"pip\", \"install\", \"-q\", \"gradio\", \"pillow\", \"accelerate\", \"transformers\"], check=True)\n\n\n\nfrom kaggle_secrets import UserSecretsClient\n\nsecrets = UserSecretsClient()\n\nhf_token = secrets.get_secret(\"HF_TOKEN\")\n\n\n\nimport torch\n\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\nimport numpy as np\n\nfrom PIL import Image\n\nimport time\n\nimport gradio as gr\n\nimport re\n\n\n\nprint(\"Gradio version:\", gr.__version__)\n\n\n\nmodel_id = \"google/medgemma-4b-it\"\n\nprint(\"Loading MedGemma 1.5...\")\n\n\n\nprocessor = AutoProcessor.from_pretrained(model_id, token=hf_token)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\n    model_id,\n\n    torch_dtype=torch.bfloat16,\n\n    device_map=\"auto\",\n\n    token=hf_token\n\n)\n\nprint(\"Model loaded on:\", next(model.parameters()).device)\n\n\n\nstop_token_id = processor.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n\n\n\nclass StopOnEndOfTurn(StoppingCriteria):\n\n    def __call__(self, input_ids, scores, **kwargs):\n\n        return input_ids[0, -1].item() == stop_token_id\n\n\n\nstop_criteria = StoppingCriteriaList([StopOnEndOfTurn()])\n\n\n\n\n\ndef truncate_at_sentence(text, max_chars=600):\n\n    # Clean up MedGemma QA artifacts (e.g. \\boxed{})\n\n    text = re.sub(r'(?i)final answer:.*', '', text)\n\n    text = re.sub(r'(?i)the final answer is.*', '', text)\n\n    text = text.replace('\\\\boxed{', '').replace('}', '')\n\n    text = text.strip()\n\n\n\n    if len(text) <= max_chars:\n\n        garbage_phrases = [\n\n            \"example reports\", \"reference purposes\",\n\n            \"Please consult\", \"Thank You\", \"Thankyou\",\n\n            \"This response provides\", \"NOT constitute\",\n\n            \"intellectual property\", \"femtogram\",\n\n            \"nanosecond\", \"election\", \"calamit\",\n\n            \"Do Not use\", \"etcetera\", \"disclaimer\",\n\n            \"Disclaimer\", \"for your reference\"\n\n        ]\n\n        for g in garbage_phrases:\n\n            if g.lower() in text.lower():\n\n                idx = text.lower().index(g.lower())\n\n                if idx > 30:\n\n                    text = text[:idx].rstrip(\" .,;:\")\n\n                    break\n\n        return text.strip()\n\n\n\n    text = text[:max_chars]\n\n    for punct in [\". \", \".\\n\", \"! \", \"? \"]:\n\n        last = text.rfind(punct)\n\n        if last > max_chars // 2:\n\n            return text[:last + 1].strip()\n\n    return text.strip()\n\n\n\n\n\ndef generate_with_image(image_pil, prompt, max_tokens=200):\n\n    messages = [\n\n        {\n\n            \"role\": \"user\",\n\n            \"content\": [\n\n                {\"type\": \"image\", \"image\": image_pil},\n\n                {\"type\": \"text\", \"text\": prompt}\n\n            ]\n\n        }\n\n    ]\n\n    inputs = processor.apply_chat_template(\n\n        messages,\n\n        add_generation_prompt=True,\n\n        tokenize=True,\n\n        return_dict=True,\n\n        return_tensors=\"pt\"\n\n    ).to(model.device)\n\n\n\n    with torch.no_grad():\n\n        output = model.generate(\n\n            **inputs,\n\n            max_new_tokens=max_tokens,\n\n            stopping_criteria=stop_criteria,\n\n            repetition_penalty=1.2,\n\n            do_sample=False\n\n        )\n\n\n\n    new_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n\n    result = processor.tokenizer.decode(new_tokens, skip_special_tokens=True)\n\n    return truncate_at_sentence(result, max_chars=600)\n\n\n\n\n\ndef generate_text_only(prompt, max_tokens=100):\n\n    input_text = \"user\\n\" + prompt + \"\\nmodel\\n\"\n\n    inputs = processor.tokenizer(\n\n        input_text, return_tensors=\"pt\"\n\n    ).to(model.device)\n\n\n\n    with torch.no_grad():\n\n        output = model.generate(\n\n            **inputs,\n\n            max_new_tokens=max_tokens,\n\n            stopping_criteria=stop_criteria,\n\n            repetition_penalty=1.2,\n\n            do_sample=False\n\n        )\n\n\n\n    new_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n\n    result = processor.tokenizer.decode(new_tokens, skip_special_tokens=True)\n\n    return truncate_at_sentence(result, max_chars=400)\n\n\n\n\n\ndef rule_based_triage(findings, context):\n\n    findings_text = findings.lower()\n\n\n\n    # Advanced sentence-level negation checker\n\n    def has_positive_mention(keywords, text):\n\n        # Split text into independent sentences/clauses\n\n        sentences = re.split(r'[.;\\n]', text)\n\n        negations = [\"no \", \"not \", \"without \", \"negative \", \"absence \", \"clear \", \"unremarkable\", \"free of\"]\n\n        \n\n        for sentence in sentences:\n\n            sentence = sentence.strip()\n\n            if not sentence: continue\n\n            \n\n            for kw in keywords:\n\n                # Look for exact word match\n\n                match = re.search(r'\\b' + re.escape(kw) + r'\\b', sentence)\n\n                if match:\n\n                    # Grab everything in the sentence BEFORE the keyword\n\n                    preceding_text = sentence[:match.start()]\n\n                    # If a negation word is anywhere before the keyword in this sentence, ignore it\n\n                    if not any(neg in preceding_text for neg in negations):\n\n                        return True\n\n        return False\n\n\n\n    # CRITICAL ðŸ”´ - Medical emergencies requiring immediate intervention\n\n    critical_keywords = [\n\n        \"tension pneumothorax\", \"massive pleural effusion\", \n\n        \"pulmonary edema\", \"edema\", \"severe pulmonary edema\",\n\n        \"aortic dissection\", \"cardiac tamponade\", \n\n        \"whiteout\", \"pneumoperitoneum\", \"tracheal deviation\"\n\n    ]\n\n    if has_positive_mention(critical_keywords, findings_text):\n\n        return \"CRITICAL\"\n\n\n\n    # URGENT ðŸŸ¡ - Acute abnormalities requiring prompt attention\n\n    urgent_keywords = [\n\n        \"consolidation\", \"pneumonia\", \"pleural effusion\", \"cardiomegaly\",\n\n        \"enlarged heart\", \"cardiac enlargement\", \"pacemaker\", \n\n        \"airspace opacity\", \"increased opacity\", \"ground glass\", \"opacities\",\n\n        \"infiltrate\", \"atelectasis\", \"mass\", \"nodule\", \"fracture\", \"pneumothorax\",\n\n        \"pleural fluid\"\n\n    ]\n\n    if has_positive_mention(urgent_keywords, findings_text):\n\n        return \"URGENT\"\n\n\n\n    # ROUTINE ðŸŸ¢ - No positive findings detected\n\n    return \"ROUTINE\"\n\n\n\n\n\ndef run_pipeline(image, patient_context):\n\n    if image is None:\n\n        return (\"Please upload an image\", \"\", \"\", \"\", \"\", \"\")\n\n\n\n    if not patient_context or patient_context.strip() == \"\":\n\n        patient_context = \"No clinical context provided\"\n\n\n\n    log = []\n\n    start_total = time.time()\n\n\n\n    try:\n\n        if isinstance(image, np.ndarray):\n\n            img_pil = Image.fromarray(image.astype(\"uint8\")).convert(\"RGB\")\n\n        else:\n\n            img_pil = image.convert(\"RGB\")\n\n        img_pil = img_pil.resize((512, 512))\n\n    except Exception as e:\n\n        return (\"Error loading image: \" + str(e), \"\", \"\", \"\", \"\", \"\")\n\n\n\n    # STEP 1\n\n    t = time.time()\n\n    log.append(\"Step 1: Extracting visual findings with MedGemma...\")\n\n    try:\n\n        findings = generate_with_image(\n\n            img_pil,\n\n            \"You are an expert radiologist. Carefully analyze this chest X-ray.\\n\"\n\n            \"Patient context: \" + patient_context + \"\\n\\n\"\n\n            \"Write exactly 5 lines, one per region. Keep it professional and brief:\\n\"\n\n            \"Lungs: [describe findings]\\n\"\n\n            \"Heart: [describe findings]\\n\"\n\n            \"Mediastinum: [describe findings]\\n\"\n\n            \"Pleura: [describe findings]\\n\"\n\n            \"Bones: [describe findings]\\n\",\n\n            250\n\n        )\n\n    except Exception as e:\n\n        findings = \"Error in visual analysis: \" + str(e)\n\n    log.append(\"Step 1 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 2\n\n    t = time.time()\n\n    log.append(\"Step 2: Classifying triage level...\")\n\n    triage_level = rule_based_triage(findings, patient_context)\n\n    triage_descriptions = {\n\n        \"CRITICAL\": \"Critical findings identified requiring immediate radiologist attention.\",\n\n        \"URGENT\": \"Significant findings identified requiring prompt radiologist review.\",\n\n        \"ROUTINE\": \"No acute findings identified. Standard reporting timeline applies.\"\n\n    }\n\n    triage_reason = triage_descriptions[triage_level]\n\n    log.append(\"Step 2 complete: \" + triage_level + \" (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 3\n\n    t = time.time()\n\n    log.append(\"Step 3: Generating radiology report with MedGemma...\")\n\n    try:\n\n        findings_paragraph = generate_text_only(\n\n            \"You are a radiologist. Write exactly 2 professional sentences \"\n\n            \"summarizing these chest X-ray findings. \"\n\n            \"Stop after 2 sentences. No extra commentary.\\n\\n\"\n\n            \"Findings: \" + findings[:350],\n\n            90\n\n        )\n\n    except Exception as e:\n\n        findings_paragraph = findings[:200]\n\n\n\n    impression_map = {\n\n        \"CRITICAL\": (\n\n            \"1. Critical cardiopulmonary finding identified.\\n\"\n\n            \"2. Immediate radiologist review and clinical intervention required.\"\n\n        ),\n\n        \"URGENT\": (\n\n            \"1. Significant cardiopulmonary finding identified.\\n\"\n\n            \"2. Prompt radiologist review and clinical correlation recommended.\"\n\n        ),\n\n        \"ROUTINE\": (\n\n            \"1. No acute cardiopulmonary abnormality identified.\\n\"\n\n            \"2. Findings within normal limits for patient age and clinical context.\"\n\n        )\n\n    }\n\n    recommendation_map = {\n\n        \"CRITICAL\": \"Immediate radiologist review and clinical intervention required.\",\n\n        \"URGENT\": \"Radiologist review recommended within 1 hour. Clinical correlation advised.\",\n\n        \"ROUTINE\": \"No immediate follow-up required. Routine clinical management.\"\n\n    }\n\n    report = (\n\n        \"**EXAMINATION:** PA chest radiograph\\n\\n\"\n\n        \"**INDICATION:** \" + patient_context + \"\\n\\n\"\n\n        \"**COMPARISON:** None available\\n\\n\"\n\n        \"**FINDINGS:** \" + findings_paragraph + \"\\n\\n\"\n\n        \"**IMPRESSION:**\\n\" + impression_map[triage_level] + \"\\n\\n\"\n\n        \"**RECOMMENDATION:** \" + recommendation_map[triage_level]\n\n    )\n\n    log.append(\"Step 3 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 4\n\n    t = time.time()\n\n    log.append(\"Step 4: Creating patient summary...\")\n\n    patient_summaries = {\n\n        \"CRITICAL\": (\n\n            \"Your chest X-ray has shown some findings that need urgent attention \"\n\n            \"from your doctor right away. \"\n\n            \"Please do not leave the hospital â€” your care team has been alerted \"\n\n            \"and will see you very soon. \"\n\n            \"This does not mean the worst, but it is important we act quickly \"\n\n            \"to keep you safe.\"\n\n        ),\n\n        \"URGENT\": (\n\n            \"Your chest X-ray has shown some findings that your doctor needs \"\n\n            \"to review soon. \"\n\n            \"This is not an emergency, but your care team will follow up with \"\n\n            \"you shortly to discuss next steps. \"\n\n            \"Please let a nurse know if your symptoms get worse while you wait.\"\n\n        ),\n\n        \"ROUTINE\": (\n\n            \"Your chest X-ray looks generally normal and no urgent problems \"\n\n            \"were found. \"\n\n            \"Your doctor will review the full results with you at your appointment. \"\n\n            \"If your symptoms change or worsen before then, please contact \"\n\n            \"your healthcare provider.\"\n\n        )\n\n    }\n\n    patient_summary = patient_summaries[triage_level]\n\n    log.append(\"Step 4 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n\n\n    # STEP 5\n\n    t = time.time()\n\n    log.append(\"Step 5: Running safety validation...\")\n\n    safety_flags = []\n\n    if triage_level == \"CRITICAL\":\n\n        safety_flags.append(\"CRITICAL - Immediate radiologist review required\")\n\n    elif triage_level == \"URGENT\":\n\n        safety_flags.append(\"URGENT - Radiologist review within 1 hour\")\n\n    else:\n\n        safety_flags.append(\"ROUTINE - Standard reporting timeline\")\n\n    safety_flags.append(\"AI-generated draft - Must be verified by a qualified radiologist\")\n\n    safety_flags.append(\"For demonstration purposes only - Not a medical device\")\n\n    log.append(\"Step 5 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n    log.append(\"Total pipeline time: \" + str(round(time.time()-start_total, 1)) + \"s\")\n\n\n\n    emoji_map = {\"CRITICAL\": \"ðŸ”´\", \"URGENT\": \"ðŸŸ¡\", \"ROUTINE\": \"ðŸŸ¢\"}\n\n    emoji = emoji_map.get(triage_level, \"âšª\")\n\n\n\n    return (\n\n        \"## \" + emoji + \" \" + triage_level + \"\\n\\n\" + triage_reason,\n\n        \"## Detailed Findings\\n\\n\" + findings,\n\n        \"## Radiology Report\\n\\n\" + report,\n\n        \"## Patient Summary\\n\\n\" + patient_summary,\n\n        \"\\n\".join(safety_flags),\n\n        \"\\n\".join(log)\n\n    )\n\n\n\n\n\nwith gr.Blocks(title=\"ChestAI Copilot\") as demo:\n\n    gr.Markdown(\n\n        \"# ChestAI Copilot\\n\"\n\n        \"### Agentic Chest X-Ray Triage and Reporting Assistant\\n\"\n\n        \"**Powered by Google MedGemma 1.5 (HAI-DEF)** | **MedGemma Impact Challenge**\\n\\n\"\n\n        \"> Demonstration only. Not a medical device. \"\n\n        \"All outputs must be verified by a qualified radiologist.\\n\\n\"\n\n        \"---\\n\\n\"\n\n        \"**How it works:** Upload a chest X-ray and the 5-step \"\n\n        \"AI agent pipeline produces Triage + Report + Patient Summary\"\n\n    )\n\n\n\n    with gr.Row():\n\n        with gr.Column(scale=1):\n\n            img_input = gr.Image(\n\n                label=\"Upload Chest X-Ray\",\n\n                type=\"numpy\",\n\n                height=300\n\n            )\n\n            context_input = gr.Textbox(\n\n                label=\"Clinical Context\",\n\n                placeholder=\"e.g., 65yo male, cough and fever x3 days, history of CHF\",\n\n                lines=3\n\n            )\n\n            gr.Markdown(\n\n                \"**Test contexts to copy/paste:**\\n\"\n\n                \"- **Normal:** 28yo female, routine pre-employment screening.\\n\"\n\n                \"- **Cardiomegaly:** 72yo female, shortness of breath, history of heart failure and pacemaker.\\n\"\n\n                \"- **Pneumonia:** 55yo male, fever, cough, right-sided chest pain x3 days.\\n\"\n\n                \"- **Edema:** 68yo male, acute severe respiratory distress, audible crackles.\"\n\n            )\n\n            submit_btn = gr.Button(\"Analyze Chest X-Ray\", variant=\"primary\")\n\n\n\n        with gr.Column(scale=2):\n\n            triage_output = gr.Markdown(label=\"Triage Level\")\n\n            safety_output = gr.Textbox(label=\"Safety Flags\", lines=3)\n\n            with gr.Tabs():\n\n                with gr.TabItem(\"Radiology Report\"):\n\n                    report_output = gr.Markdown()\n\n                with gr.TabItem(\"Detailed Findings\"):\n\n                    findings_output = gr.Markdown()\n\n                with gr.TabItem(\"Patient Summary\"):\n\n                    patient_output = gr.Markdown()\n\n                with gr.TabItem(\"Pipeline Log\"):\n\n                    log_output = gr.Textbox(lines=10, label=\"Agent Execution Log\")\n\n\n\n    gr.Markdown(\n\n        \"---\\n\"\n\n        \"### Architecture\\n\"\n\n        \"```\\n\"\n\n        \"Upload CXR -> [Agent 1: Visual Analysis (MedGemma)] \"\n\n        \"-> [Agent 2: Triage Classification] \"\n\n        \"-> [Agent 3: Report Generation (MedGemma)] \"\n\n        \"-> [Agent 4: Patient Summary] \"\n\n        \"-> [Agent 5: Safety Validation] -> Dashboard\\n\"\n\n        \"```\\n\\n\"\n\n        \"**Model:** MedGemma 1.5 4B-IT (multimodal) from Google HAI-DEF\\n\\n\"\n\n        \"*Built for the MedGemma Impact Challenge on Kaggle*\"\n\n    )\n\n\n\n    submit_btn.click(\n\n        fn=run_pipeline,\n\n        inputs=[img_input, context_input],\n\n        outputs=[\n\n            triage_output,\n\n            findings_output,\n\n            report_output,\n\n            patient_output,\n\n            safety_output,\n\n            log_output\n\n        ]\n\n    )\n\n\n\ndemo.queue().launch(share=True, debug=True, show_error=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T18:19:23.824777Z","iopub.execute_input":"2026-02-24T18:19:23.825286Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: gradio 6.6.0\nUninstalling gradio-6.6.0:\n  Successfully uninstalled gradio-6.6.0\nFound existing installation: gradio_client 2.1.0\nUninstalling gradio_client-2.1.0:\n  Successfully uninstalled gradio_client-2.1.0\nGradio version: 6.6.0\nLoading MedGemma 1.5...\n","output_type":"stream"},{"name":"stderr","text":"The image processor of type `Gemma3ImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0c78e82153544788a588d5ab3cbc184"}},"metadata":{}},{"name":"stdout","text":"Model loaded on: cuda:0\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://e1845c7b38541fffa3.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://e1845c7b38541fffa3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":118534,"databundleVersionId":14898831},{"sourceType":"modelInstanceVersion","sourceId":720206,"databundleVersionId":15326820,"modelInstanceId":547777,"modelId":480416}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import subprocess\nsubprocess.run(\n    [\"pip\", \"install\", \"-q\", \"gradio\", \"pillow\", \"accelerate\", \"transformers\"],\n    check=True\n)\n\nfrom kaggle_secrets import UserSecretsClient\nsecrets = UserSecretsClient()\nhf_token = secrets.get_secret(\"HF_TOKEN\")\n\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\nmodel_id = \"google/medgemma-4b-it\"\nprint(\"Loading MedGemma 1.5...\")\n\nprocessor = AutoProcessor.from_pretrained(model_id, token=hf_token)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    token=hf_token\n)\nprint(\"Model loaded on:\", next(model.parameters()).device)\n\nstop_token_id = processor.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n\nclass StopOnEndOfTurn(StoppingCriteria):\n    def __call__(self, input_ids, scores, **kwargs):\n        return input_ids[0, -1].item() == stop_token_id\n\nstop_criteria = StoppingCriteriaList([StopOnEndOfTurn()])\n\n\ndef truncate_at_sentence(text, max_chars=400):\n    \"\"\"Cut at last complete sentence within max_chars\"\"\"\n    if len(text) <= max_chars:\n        # Still check for garbage\n        garbage_phrases = [\n            \"example reports\", \"reference purposes\",\n            \"Please consult\", \"Thank You\", \"Thankyou\",\n            \"This response provides\", \"NOT constitute\",\n            \"intellectual property\", \"femtogram\",\n            \"nanosecond\", \"election\", \"calamit\",\n            \"Do Not use\", \"etcetera\"\n        ]\n        for g in garbage_phrases:\n            if g.lower() in text.lower():\n                idx = text.lower().index(g.lower())\n                if idx > 30:\n                    text = text[:idx].rstrip(\" .,;:\")\n                    break\n        return text.strip()\n\n    # Hard cut at max_chars then find last sentence end\n    text = text[:max_chars]\n    for punct in [\". \", \".\\n\", \"! \", \"? \"]:\n        last = text.rfind(punct)\n        if last > max_chars // 2:\n            return text[:last + 1].strip()\n    return text.strip()\n\n\ndef generate_with_image(image_pil, prompt, max_tokens=200):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": image_pil},\n                {\"type\": \"text\", \"text\": prompt}\n            ]\n        }\n    ]\n    inputs = processor.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            stopping_criteria=stop_criteria,\n            repetition_penalty=1.8,\n            do_sample=False\n        )\n\n    new_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n    result = processor.tokenizer.decode(new_tokens, skip_special_tokens=True)\n    return truncate_at_sentence(result, max_chars=600)\n\n\ndef generate_text_only(prompt, max_tokens=100):\n    input_text = \"user\\n\" + prompt + \"\\nmodel\\n\"\n    inputs = processor.tokenizer(\n        input_text, return_tensors=\"pt\"\n    ).to(model.device)\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            stopping_criteria=stop_criteria,\n            repetition_penalty=1.8,\n            do_sample=False\n        )\n\n    new_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n    result = processor.tokenizer.decode(new_tokens, skip_special_tokens=True)\n    return truncate_at_sentence(result, max_chars=400)\n\n\nimport gradio as gr\nfrom PIL import Image\nimport time\n\n\ndef run_pipeline(image, patient_context):\n    if image is None:\n        return (\"Please upload an image\", \"\", \"\", \"\", \"\", \"\")\n\n    if not patient_context:\n        patient_context = \"No clinical context provided\"\n\n    log = []\n    start_total = time.time()\n    img_pil = Image.fromarray(image).convert(\"RGB\").resize((512, 512))\n\n    # STEP 1: Visual Findings\n    t = time.time()\n    log.append(\"Step 1: Extracting visual findings...\")\n\n    findings = generate_with_image(\n        img_pil,\n        \"You are a radiologist. List findings for this chest X-ray.\\n\"\n        \"Patient: \" + patient_context + \"\\n\"\n        \"Write exactly 5 lines:\\n\"\n        \"Lungs: [finding]\\n\"\n        \"Heart: [finding]\\n\"\n        \"Mediastinum: [finding]\\n\"\n        \"Pleura: [finding]\\n\"\n        \"Bones: [finding]\\n\"\n        \"Use 'Normal' if no abnormality. Stop after Bones line.\",\n        200\n    )\n\n    log.append(\"Step 1 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n    # STEP 2: Triage â€” classify only, no generation for reason\n    t = time.time()\n    log.append(\"Step 2: Classifying triage level...\")\n\n    triage_raw = generate_text_only(\n        \"Reply with ONE word only: CRITICAL, URGENT, or ROUTINE.\\n\"\n        \"CRITICAL = tension pneumothorax, massive effusion, severe pulmonary edema.\\n\"\n        \"URGENT = pneumonia, moderate effusion, new mass, new cardiomegaly.\\n\"\n        \"ROUTINE = no acute findings.\\n\"\n        \"Findings: \" + findings[:300] + \"\\n\"\n        \"One word:\",\n        5\n    )\n\n    triage_level = \"ROUTINE\"\n    check = triage_raw.upper().strip()\n    if \"CRITICAL\" in check:\n        triage_level = \"CRITICAL\"\n    elif \"URGENT\" in check:\n        triage_level = \"URGENT\"\n\n    # Build reason from findings â€” no generation needed\n    triage_descriptions = {\n        \"CRITICAL\": \"Critical findings identified requiring immediate radiologist attention.\",\n        \"URGENT\": \"Significant findings identified requiring prompt radiologist review.\",\n        \"ROUTINE\": \"No acute findings identified. Standard reporting timeline applies.\"\n    }\n    triage_reason = triage_descriptions[triage_level]\n\n    log.append(\"Step 2 complete: \" + triage_level + \" (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n    # STEP 3: Report body â€” only generate FINDINGS paragraph\n    t = time.time()\n    log.append(\"Step 3: Generating radiology report...\")\n\n    findings_paragraph = generate_text_only(\n        \"Write 2 sentences summarizing these chest X-ray findings \"\n        \"in professional radiology language. Stop after 2 sentences.\\n\"\n        \"Findings: \" + findings[:300],\n        80\n    )\n\n    impression_map = {\n        \"CRITICAL\": \"1. Critical cardiopulmonary finding identified.\\n2. Immediate radiologist review required.\",\n        \"URGENT\": \"1. Significant finding requiring prompt attention.\\n2. Clinical correlation recommended.\",\n        \"ROUTINE\": \"1. No acute cardiopulmonary abnormality identified.\\n2. Findings within normal limits for age.\"\n    }\n\n    recommendation_map = {\n        \"CRITICAL\": \"Immediate radiologist review and clinical intervention required.\",\n        \"URGENT\": \"Radiologist review recommended within 1 hour. Clinical correlation advised.\",\n        \"ROUTINE\": \"No immediate follow-up required. Routine clinical management.\"\n    }\n\n    report = (\n        \"**EXAMINATION:** PA chest radiograph\\n\\n\"\n        \"**INDICATION:** \" + patient_context + \"\\n\\n\"\n        \"**COMPARISON:** None available\\n\\n\"\n        \"**FINDINGS:** \" + findings_paragraph + \"\\n\\n\"\n        \"**IMPRESSION:**\\n\" + impression_map[triage_level] + \"\\n\\n\"\n        \"**RECOMMENDATION:** \" + recommendation_map[triage_level]\n    )\n\n    log.append(\"Step 3 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n    # STEP 4: Patient Summary â€” build from template, no generation\n    t = time.time()\n    log.append(\"Step 4: Creating patient summary...\")\n\n    patient_summaries = {\n        \"CRITICAL\": (\n            \"Your chest X-ray has shown some findings that need urgent attention from your doctor right away. \"\n            \"Please do not leave the hospital â€” your care team has been alerted and will see you very soon. \"\n            \"This does not mean the worst, but it is important we act quickly to keep you safe.\"\n        ),\n        \"URGENT\": (\n            \"Your chest X-ray has shown some findings that your doctor needs to review soon. \"\n            \"This is not an emergency, but your care team will follow up with you shortly to discuss next steps. \"\n            \"Please let a nurse know if your symptoms get worse while you wait.\"\n        ),\n        \"ROUTINE\": (\n            \"Your chest X-ray looks generally normal and no urgent problems were found. \"\n            \"Your doctor will review the full results with you at your appointment. \"\n            \"If your symptoms change or worsen before then, please contact your healthcare provider.\"\n        )\n    }\n    patient_summary = patient_summaries[triage_level]\n\n    log.append(\"Step 4 complete (\" + str(round(time.time()-t, 1)) + \"s)\")\n\n    # STEP 5: Safety flags\n    safety_flags = []\n    if triage_level == \"CRITICAL\":\n        safety_flags.append(\"ðŸ”´ CRITICAL - Immediate radiologist review required\")\n    elif triage_level == \"URGENT\":\n        safety_flags.append(\"ðŸŸ¡ URGENT - Radiologist review within 1 hour\")\n    else:\n        safety_flags.append(\"ðŸŸ¢ ROUTINE - Standard reporting timeline\")\n    safety_flags.append(\"âš ï¸ AI-generated draft - Must be verified by a qualified radiologist\")\n    safety_flags.append(\"âš ï¸ For demonstration purposes only - Not a medical device\")\n\n    log.append(\"Total pipeline time: \" + str(round(time.time()-start_total, 1)) + \"s\")\n\n    emoji_map = {\"CRITICAL\": \"ðŸ”´\", \"URGENT\": \"ðŸŸ¡\", \"ROUTINE\": \"ðŸŸ¢\"}\n    emoji = emoji_map.get(triage_level, \"âšª\")\n\n    return (\n        \"## \" + emoji + \" \" + triage_level + \"\\n\\n\" + triage_reason,\n        \"## Detailed Findings\\n\\n\" + findings,\n        \"## Radiology Report\\n\\n\" + report,\n        \"## Patient Summary\\n\\n\" + patient_summary,\n        \"\\n\".join(safety_flags),\n        \"\\n\".join(log)\n    )\n\n\nwith gr.Blocks(title=\"ChestAI Copilot\") as demo:\n\n    gr.Markdown(\n        \"# ðŸ« ChestAI Copilot\\n\"\n        \"### Agentic Chest X-Ray Triage and Reporting Assistant\\n\"\n        \"**Powered by Google MedGemma 1.5 (HAI-DEF)** | **MedGemma Impact Challenge**\\n\\n\"\n        \"> âš ï¸ Demonstration only. Not a medical device. \"\n        \"All outputs must be verified by a qualified radiologist.\\n\\n\"\n        \"---\\n\\n\"\n        \"**How it works:** Upload a chest X-ray then the 5-step \"\n        \"AI agent pipeline produces Triage + Report + Patient Summary\"\n    )\n\n    with gr.Row():\n        with gr.Column(scale=1):\n            img_input = gr.Image(label=\"ðŸ“¤ Upload Chest X-Ray\", type=\"numpy\", height=350)\n            context_input = gr.Textbox(\n                label=\"ðŸ“‹ Clinical Context\",\n                placeholder=\"e.g., 65yo male, cough and fever x3 days, history of CHF\",\n                lines=3\n            )\n            gr.Markdown(\n                \"**Test contexts:**\\n\"\n                \"- 72yo female, shortness of breath, history of heart failure\\n\"\n                \"- 45yo male, chest pain after car accident\\n\"\n                \"- 28yo female, routine pre-employment screening\"\n            )\n            submit_btn = gr.Button(\"ðŸ” Analyze Chest X-Ray\", variant=\"primary\", size=\"lg\")\n\n        with gr.Column(scale=2):\n            with gr.Row():\n                triage_output = gr.Markdown(label=\"Triage Level\")\n                safety_output = gr.Textbox(label=\"Safety Flags\", lines=4)\n            with gr.Tabs():\n                with gr.TabItem(\"ðŸ“‹ Radiology Report\"):\n                    report_output = gr.Markdown()\n                with gr.TabItem(\"ðŸ” Detailed Findings\"):\n                    findings_output = gr.Markdown()\n                with gr.TabItem(\"ðŸ’¬ Patient Summary\"):\n                    patient_output = gr.Markdown()\n                with gr.TabItem(\"ðŸ“ Pipeline Log\"):\n                    log_output = gr.Textbox(lines=10, label=\"Agent Execution Log\")\n\n    gr.Markdown(\n        \"---\\n\"\n        \"### ðŸ—ï¸ Architecture\\n\"\n        \"```\\n\"\n        \"Upload CXR -> [Agent 1: Visual Analysis] -> [Agent 2: Triage] \"\n        \"-> [Agent 3: Report] -> [Agent 4: Patient Summary] \"\n        \"-> [Agent 5: Safety Check] -> Dashboard\\n\"\n        \"```\\n\\n\"\n        \"**Model:** MedGemma 1.5 4B-IT (multimodal) from Google HAI-DEF\\n\\n\"\n        \"*Built for the MedGemma Impact Challenge on Kaggle*\"\n    )\n\n    submit_btn.click(\n        fn=run_pipeline,\n        inputs=[img_input, context_input],\n        outputs=[triage_output, findings_output, report_output,\n                 patient_output, safety_output, log_output]\n    )\n\ndemo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T03:02:20.311205Z","iopub.execute_input":"2026-02-23T03:02:20.311495Z","execution_failed":"2026-02-23T04:38:14.166Z"}},"outputs":[],"execution_count":null}]}